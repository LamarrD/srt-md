1
00:00:00,000 --> 00:00:03,620
Now, let's discuss a few examples of classification models.

2
00:00:03,620 --> 00:00:05,785
The first one is decision tree,

3
00:00:05,785 --> 00:00:09,060
here the training data is repeatedly petitioned until

4
00:00:09,060 --> 00:00:12,415
all examples in each petition belong to one class.

5
00:00:12,415 --> 00:00:14,760
The decision tree can also be thought of as

6
00:00:14,760 --> 00:00:17,580
a set of rules that describe the decision logic.

7
00:00:17,580 --> 00:00:19,540
Let's go over a simple example,

8
00:00:19,540 --> 00:00:21,450
the application here is to learn

9
00:00:21,450 --> 00:00:24,775
the decision tree to decide whether we should play tennis or not.

10
00:00:24,775 --> 00:00:28,555
So the class label here is yes to no to play tennis,

11
00:00:28,555 --> 00:00:32,150
and the features or attributes include the weather conditions,

12
00:00:32,150 --> 00:00:35,925
and here is the decision tree produced by a machine learning algorithm.

13
00:00:35,925 --> 00:00:39,655
Again, a decision tree can be represented as a set of rules.

14
00:00:39,655 --> 00:00:43,980
For example, if you see outlook equal to sunny and humidity equal to normal,

15
00:00:43,980 --> 00:00:45,755
then yes we play tennis.

16
00:00:45,755 --> 00:00:49,670
If the outlook is overcast then also yes to play tennis,

17
00:00:49,670 --> 00:00:52,430
or if the outlook is rain and wind is weak,

18
00:00:52,430 --> 00:00:54,485
then we also say yes to play tennis.

19
00:00:54,485 --> 00:00:58,370
Here's a high level overview of the process of building a decision tree.

20
00:00:58,370 --> 00:01:01,875
We first find the best attributes that can serve as the root.

21
00:01:01,875 --> 00:01:04,070
Then we break-up the training dataset into

22
00:01:04,070 --> 00:01:07,340
subsets based on the values of this root attribute,

23
00:01:07,340 --> 00:01:10,170
that is, we grow from the root into branches.

24
00:01:10,170 --> 00:01:13,060
Therefore each subset we test the remaining attributes to

25
00:01:13,060 --> 00:01:16,250
see which is best at petitioning this subset,

26
00:01:16,250 --> 00:01:17,955
that is, for each branch,

27
00:01:17,955 --> 00:01:22,125
we test attributes to determine how to best grow into branches.

28
00:01:22,125 --> 00:01:27,070
We continue this process until all examples in a subset belong to one class,

29
00:01:27,070 --> 00:01:30,555
or there's no more attributes left to further permission to subset.

30
00:01:30,555 --> 00:01:33,760
In this case, the default cast label is the majority.

31
00:01:33,760 --> 00:01:36,145
Now the question is, how do we determine which

32
00:01:36,145 --> 00:01:39,835
attribute is best at partitioning a set into subsets?

33
00:01:39,835 --> 00:01:43,285
We use entropy and information gain to determine this.

34
00:01:43,285 --> 00:01:46,450
The entropy of a set is the minimal number of bits needed to

35
00:01:46,450 --> 00:01:49,855
represent examples in this set according to their class labels.

36
00:01:49,855 --> 00:01:52,580
Therefore, roughly speaking, the entropy of a set

37
00:01:52,580 --> 00:01:55,610
represents how pure the examples are in this set.

38
00:01:55,610 --> 00:02:00,140
That is, if the examples of this set are evenly distributed into different classes,

39
00:02:00,140 --> 00:02:04,100
then the entropy is the maximum and if the examples are all in a single class,

40
00:02:04,100 --> 00:02:05,720
then the entropy is the minimum.

41
00:02:05,720 --> 00:02:09,920
In order to determine which attribute is best at partitioning a set,

42
00:02:09,920 --> 00:02:11,935
we compute its information gain.

43
00:02:11,935 --> 00:02:13,655
As you can see from this formula,

44
00:02:13,655 --> 00:02:16,025
a high information gain value for attribute A,

45
00:02:16,025 --> 00:02:18,335
means that A is better as separating

46
00:02:18,335 --> 00:02:21,510
samples in a set S according to their classification,

47
00:02:21,510 --> 00:02:26,330
that is, using A S a petition into purer subsets.

48
00:02:26,330 --> 00:02:28,760
Therefore, we should select the attribute that has

49
00:02:28,760 --> 00:02:32,000
the highest information gain and use it to partition a set.

